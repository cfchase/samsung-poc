{
 "cells": [
  {
   "cell_type": "code",
   "id": "cd700427e4c2b4b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install llama-index\n",
    "!pip install langchain\n",
    "!pip install langchain-community\n",
    "!pip install langchain-openai\n",
    "!pip install llama-index-llms-huggingface\n",
    "!pip install llama-index-llms-huggingface-api\n",
    "!pip install llama-index-embeddings-huggingface-api\n",
    "!pip install ragas\n",
    "!pip install pymilvus\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "tags": []
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset, load_dataset\n",
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "data_dir = os.getenv('DATA_DIR', \"data\")\n",
    "testset_json = os.getenv(\"TESTSET_JSON\", \"testset.jsonl\")\n",
    "\n",
    "infer_endpoint = os.getenv(\"INFER_ENDPOINT\")\n",
    "model_name = os.getenv(\"MODEL_NAME\")\n",
    "api_key = os.getenv(\"LLM_API_KEY\")\n",
    "\n",
    "loader = DirectoryLoader(data_dir, loader_cls=TextLoader, glob=\"**/*.md\")\n",
    "documents = loader.load()\n",
    "testset = Dataset.from_json(testset_json)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "639be309-58ac-4039-ad4d-44a2f4612027",
   "metadata": {
    "tags": []
   },
   "source": [
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# LLM definition\n",
    "llm = VLLMOpenAI(           # we are using the vLLM OpenAI-compatible API client. But the Model is running on OpenShift, not OpenAI.\n",
    "    openai_api_key=api_key,   # and that is why we don't need an OpenAI key for this.\n",
    "    openai_api_base= f\"{infer_endpoint}/v1\",\n",
    "    model_name=f\"{model_name}\",\n",
    "    temperature=0.00,\n",
    "    max_tokens=2048\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9cb300c3-0d67-419f-98bf-e1433155e695",
   "metadata": {
    "tags": []
   },
   "source": [
    "TEXT_QA_TEMPLATE = (\n",
    "    \"<|system|>\\n\"\n",
    "    \"You are, Red Hat Instruct Model based on Granite 7B, an AI language model developed by Red Hat and IBM Research, based on the Granite-7b-base language model. My primary function is to be a chat assistant.\\n\"\n",
    "    \"<|user|>\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"Given the above context information, answer the query.\\n\"\n",
    "    \"{query_str}\\n\"\n",
    "    \"<|assistant|>\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"query_str\", \"context_str\"], template=TEXT_QA_TEMPLATE)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "def generate_answers(testset, prompt, llm):\n",
    "    dataset_df = testset.to_pandas()\n",
    "    dataset_df[\"answer\"] = None\n",
    "    \n",
    "    for index, row in dataset_df.iterrows():\n",
    "        conversation = LLMChain(\n",
    "            llm=llm,\n",
    "            prompt=prompt,\n",
    "            verbose=False\n",
    "        )\n",
    "        query_str = row[\"question\"]\n",
    "        context_str = \"\\n\\n\".join(row[\"contexts\"])\n",
    "        answer = conversation.predict(query_str=query_str, context_str=context_str)\n",
    "        print(answer[:50])\n",
    "        dataset_df.loc[index, \"answer\"] = answer\n",
    "        \n",
    "    return Dataset.from_pandas(dataset_df)"
   ],
   "id": "764ae3450c3c4cef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "answers_ds = generate_answers(testset, PROMPT, llm)\n",
    "answers_ds.to_pandas()"
   ],
   "id": "da6ff9189fad2611",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "11d4937c975ba892",
   "metadata": {},
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "eval_result = evaluate(\n",
    "    answers_ds,\n",
    "    metrics=metrics,\n",
    ")\n",
    "eval_result"
   ],
   "id": "ee8cb0657371114d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "54e20bb836f9e90",
   "metadata": {},
   "source": "eval_result.to_pandas()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f229c5a7458d2096",
   "metadata": {},
   "source": "eval_result.to_pandas().to_csv(\"eval_result.csv\", index=False, header=True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2f5cffd72bdff708",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
